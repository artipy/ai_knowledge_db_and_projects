# Развертывание и использование локальных LLM с LM Studio

Эта заметка описывает процесс развертывания и взаимодействия с локальными крупными языковыми моделями (LLM) с использованием LM Studio. Это позволяет запускать мощные модели прямо на вашем компьютере, используя их через простой API, совместимый с OpenAI.

## Связанные заметки
- [[LLMOps/Introduction_to_LLMOps_and_Ideation_Phase]] — выбор между open-source и проприетарными моделями в рамках LLMOps
- [[Hugging_Face/Getting_Started_with_Hugging_Face]] — альтернативный способ работы с open-source моделями через Hugging Face
- [[Prompt_Engineering/Основы_Prompt_Engineering]] - техники создания эффективных промптов для локальных моделей
- [[OpenAI_API/Working_with_OpenAI_API_in_Python]] - работа с OpenAI API (синтаксис совместим с LM Studio)
- [[Python_for_AI/Tenacity_Library_for_Retry_Logic|Библиотека Tenacity для retry-логики]] - обработка ошибок при работе с локальными моделями

## Установка и запуск LM Studio

1.  **Загрузка и установка:** Скачайте и установите LM Studio с официального сайта: [https://lmstudio.ai/download](https://lmstudio.ai/download). Приложение доступно для различных операционных систем.
2.  **Поиск и выбор модели:** После запуска LM Studio перейдите на вкладку поиска моделей. Вы можете просматривать доступные модели, включая различные версии от сообщества.
3.  **Загрузка модели:** Выберите желаемую модель и нажмите кнопку "Download". Рекомендуется выбирать версии с квантованием Q4_K_M или Q8_0, особенно для моделей с небольшим количеством параметров (например, 1B, которые займут всего 1-2 ГБ памяти).
    *   **Квантование:** Чем выше уровень квантования (например, Q8_0 лучше Q4_K_M по качеству, но потребляет больше ресурсов), тем выше точность и качество ответов модели, но при этом возрастает потребление оперативной памяти и нагрузка на процессор/GPU.
    *   **Ограничения:** При слишком высоком уровне квантования или недостатке системных ресурсов модель может не запуститься или работать крайне медленно.
4.  **Запуск локального сервера:** Откройте вкладку "Developer" (иконка терминала на левой панели).
5.  **Выбор загруженной модели:** В верхней части экрана выберите ранее скачанную модель из выпадающего списка.
6.  **Проверка статуса сервера:** Убедитесь, что в логах LM Studio появилось сообщение `Started HTTP server on port 1234`. Это подтверждает успешный запуск локального API-сервера.

## Использование модели через Python (OpenAI API)

Для взаимодействия с локальной моделью через Python можно использовать библиотеку `openai`, так как LM Studio предоставляет API, совместимый с OpenAI.

1.  **Установка библиотеки `openai`:**
    ```bash
    pip install openai
    ```
2.  **Пример кода для подключения и запроса:**

    ```python
    from openai import OpenAI

    # Настройка клиента для подключения к локальному серверу LM Studio
    # base_url указывает на адрес локального сервера LM Studio
    # api_key может быть любым значением, например "lm-studio", так как на локальном сервере аутентификация не требуется.
    client = OpenAI(base_url="http://localhost:1234/v1", api_key="lm-studio")

    # Выполнение запроса к модели
    completion = client.chat.completions.create(
      model="llama-3.2-1b", # Имя модели здесь может быть произвольным, LM Studio использует текущую загруженную модель
      messages=[
        {"role": "system", "content": "Ты полезный ассистент. Отвечай кратко и по делу."},
        {"role": "user", "content": "Расскажи в двух предложениях, почему небо голубое?"}
      ],
      temperature=0.7, # Параметр, контролирующий "креативность" ответов (от 0 до 1, где 1 - максимально креативно)
    )

    # Вывод ответа модели
    print(completion.choices[0].message.content)
    ```

## Полезные советы и оптимизации

*   **GPU Offload (для Mac M1/M2/M3):** В настройках локального сервера LM Studio (правая боковая панель) убедитесь, что функция "GPU Offload" включена или установлено значение "Metal". Это позволяет модели использовать графический процессор вашего Mac, что значительно ускоряет генерацию ответов.
*   **Длина контекста (Context Length):** Для моделей с небольшим количеством параметров (например, 1B) и ограниченным объемом оперативной памяти не устанавливайте слишком большое значение "Context Length" (например, выше 2048-4096). Чрезмерная длина контекста может привести к значительному замедлению работы или переполнению памяти.
*   **Keep Alive:** В настройках сервера можно установить время, в течение которого модель будет оставаться загруженной в память после последнего запроса. Это позволяет избежать повторной загрузки модели при частом использовании, сокращая время отклика.
*   **Безопасность и сетевой доступ:** По умолчанию локальный сервер LM Studio (localhost:1234) доступен только с вашего компьютера. Если вам требуется доступ к модели с других устройств в той же локальной сети (например, по Wi-Fi), измените адрес хоста в настройках LM Studio с `127.0.0.1` на `0.0.0.0`. Будьте осторожны, так как это открывает доступ к серверу извне.

Эта настройка предоставляет удобный способ для экспериментов и разработки с LLM, используя преимущества локального развертывания.
