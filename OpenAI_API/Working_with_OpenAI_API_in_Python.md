# Работа с OpenAI API в Python

Эта заметка содержит практические примеры работы с OpenAI API для различных задач обработки текста: редактирование, резюмирование, генерация текста и техники shot prompting.

## Связанные заметки

- [[OpenAI_API/Introduction_to_OpenAI_API|Введение в OpenAI API]] - базовая информация о подключении и работе с API
- [[OpenAI_API/Chat_Roles_and_Multi_Turn_Conversations|Роли в чатах и многоэтапные диалоги]] - использование ролей для управления поведением, создание диалогов с контекстом
- [[Exercises/01_OpenAI_API_Text_Processing_Exercise|Практическое задание №1]] - задания для проверки усвоенных знаний по работе с API

## Подготовка к работе

Для работы с примерами необходимо установить библиотеки и настроить подключение:

```python
from openai import OpenAI
from dotenv import load_dotenv
import os

load_dotenv()

client = OpenAI(
    api_key=os.getenv("GEMINI_TOKEN"),
    base_url=os.getenv("BASE_URL")
)
```

## Редактирование текста (Text Editing)

Модель можно использовать для изменения существующих текстов. Промпт начинается с инструкций, а после указывается текст, который следует изменить. Используйте тройные обратные кавычки для явного выделения изменяемого текста.

### Пример: Изменение имени, рода и должности

```python
prompt = """Обнови имя на Мартин, род на мужской, и наименование должности на Senior Content Developer в следующем тексте:

Джоан является Content Developer-ом в компании DataCamp. Ее любимый язык программирования R, который она использует для статистического анализа"""

response = client.chat.completions.create(
    model=os.getenv("BASE_MODEL"),
    messages=[
        {"role": "user", "content": prompt}
    ]
)

print(response.choices[0].message.content)
```

**Результат:**
```
Мартин является Senior Content Developer-ом в компании DataCamp. Его любимый язык программирования R, который он использует для статистического анализа.
```

## Резюмирование текстов (Text Summarization)

Представьте, что вы работаете в большой компании с миллионом пользователей. Отдел поддержки попросил составить краткое резюме чатов с пользователями.

### Пример: Резюмирование диалога службы поддержки

```python
text = """
Пользователь: Привет, я пытался войти в свой аккаунт, но система каждый раз говорит о том что я ввожу неправильный пароль. Я уверен что ввожу все верно.

Поддержка: Здравствуйте! Нам очень жаль слышать это. Подскажите вы пробовали сбросить пароль?
"""

prompt = f"""
Резюмируй следующий диалог между пользователем и службой поддержки в три ключевых момента: {text}
"""

response = client.chat.completions.create(
    model=os.getenv("BASE_MODEL"),
    messages=[
        {"role": "user", "content": prompt}
    ]
)

print(response.choices[0].message.content)
```

**Результат:**
```
1. **Проблема с доступом:** Пользователь не может войти в аккаунт из-за ошибки «неверный пароль».
2. **Уверенность пользователя:** Клиент утверждает, что вводит все данные правильно.
3. **Рекомендация поддержки:** Специалист предложил пользователю попробовать сбросить пароль для решения проблемы.
```

Таким образом мы получили краткое содержание главных пунктов чата поддержки без необходимости читать весь чат целиком. Представьте насколько это может быть полезно, когда таких чатов тысячи.

## Управление длиной ответа и расчет стоимости

### Параметр max_completion_tokens

По умолчанию ответ модели ограничивается только лишь ограничениями API. Но существует параметр `max_completion_tokens`, который позволяет контролировать длину ответа самостоятельно.

**Токены** - это единицы, в которых измеряются одно или несколько букв. Токены используются LLM для понимания вводимого текста. `max_completion_tokens` - это верхняя граница, за которую не может выходить модель при возврате ответа.

### Стоимость запросов

Каждая модель имеет свои цены на входные и выходные токены, причем входные и выходные токены могут иметь разную цену. Увеличение параметра `max_completion_tokens` напрямую влияет на цену каждого запроса к модели.

### Пример: Расчет стоимости запроса

```python
text = """
Пользователь: Привет, я пытался войти в свой аккаунт, но система каждый раз говорит о том что я ввожу неправильный пароль. Я уверен что ввожу все верно.

Поддержка: Здравствуйте! Нам очень жаль слышать это. Подскажите вы пробовали сбросить пароль?
"""

prompt = f"""
Резюмируй следующий диалог между пользователем и службой поддержки в три ключевых момента: {text}
"""

max_completion_tokens = 500

response = client.chat.completions.create(
    model=os.getenv("BASE_MODEL"),
    messages=[
        {"role": "user", "content": prompt}
    ],
    max_completion_tokens=max_completion_tokens
)

# Для начала рассчитываем стоимость одного входного и выходного токена
input_token_price = 0.15 / 1000000
output_token_price = 0.60 / 1000000

# Находим количество входных и выходных токенов
input_tokens = response.usage.prompt_tokens
# Так как мы установили верхнюю границу на выходные токены, используем это значение
output_tokens = max_completion_tokens

# Рассчитываем общую стоимость
cost = (input_tokens * input_token_price) + (output_tokens * output_token_price)

print(f'Общая стоимость запроса: ${cost:.8f}')
```

**Результат:**
```
Общая стоимость запроса: $0.00031485
```

В целом стоимость получилась небольшой, но если передать модели большой документ для резюмирования или анализа, стоимость сразу увеличится в разы.

## Генерация текста (Text Generation)

Когда мы отправляем промпт модели, модель возвращает текст, который с большей достоверностью подходит предоставленному промпту. Это в конечном итоге сильно зависит от данных, на которых модель обучалась.

### Пример: Продолжение предложения

```python
prompt = """
Жизнь похожа на коробку шоколадных конфет.
"""

response = client.chat.completions.create(
    model=os.getenv("BASE_MODEL"),
    messages=[
        {"role": "user", "content": prompt}
    ]
)

print(response.choices[0].message.content)
```

Модель самостоятельно нашла взаимосвязь между предложением и знаменитым фильмом "Форрест Гамп" и продолжила цитату. Однако мы не можем гарантировать, что ответ будет всегда одинаковым, потому что ответы модели не детерминированы.

### Параметр temperature

Существует множество случаев, когда модель должна отвечать более определенно. Для этого используется специальный параметр **`temperature`**, который может принимать значения от 0 до 2 и по умолчанию установлен в 1:

- **0** - наиболее детерминированный ответ
- **1** - сбалансированный (по умолчанию)
- **2** - абсолютно случайный, творческий ответ

### Пример с высокой температурой

```python
prompt = """
Жизнь похожа на коробку шоколадных конфет.
"""

response = client.chat.completions.create(
    model=os.getenv("BASE_MODEL"),
    messages=[
        {"role": "user", "content": prompt}
    ],
    temperature=2
)

print(response.choices[0].message.content)
```

С `temperature=2` ответ получается более творческим и разнообразным.

### Пример: Генерация лозунга для продукта

```python
prompt = """
Сгенерируй сильный лозунг для нового электромобиля, лозунг должен подчеркивать инновационность, экологичность и передовые технологии автомобиля.
"""

response = client.chat.completions.create(
    model=os.getenv("BASE_MODEL"),
    messages=[
        {"role": "user", "content": prompt}
    ]
)

print(response.choices[0].message.content)
```

### Пример: Генерация описания продукта

```python
prompt = """
Напиши убедительное описание продукта для UltraFit Smartwatch. Выдели особенности:
- До 10 дней работы от батареи
- Мониторинг сна и сердечного ритма 24/7
- Встроенный GPS и водонепроницаемость до 50 метров
- Легковесный дизайн и стильный внешний вид

Используй убедительный и вдохновляющий тон, чтобы привлечь потенциальных покупателей, фитнес-энтузиастов и тех, кто ищет надежный и стильный смарт-часовой аксессуар.
"""

response = client.chat.completions.create(
    model=os.getenv("BASE_MODEL"),
    messages=[
        {"role": "user", "content": prompt}
    ]
)

print(response.choices[0].message.content)
```

## Shot Prompting

Shot prompting - это техника, которая подразумевает предоставление модели некоторых примеров, которыми модель будет руководствоваться при ответе.

Существует несколько подходов использования shot prompting:

- **Zero-shot**: не предоставляется ни одного примера (single-turn task)
- **One-shot**: дается один пример, полезно для того чтобы показать модели структуру результата, которую мы хотели бы увидеть в ответе
- **Few-shot**: дается более одного примера, полезно когда необходимо решить задачу классификации или категоризации; в таком случае в примерах модели показываются примеры и их класс или категория

**Примечание**: Shot prompting также можно реализовать через систему ролей (user-assistant диалог) для более структурированного подхода. Подробнее см. [[OpenAI_API/Chat_Roles_and_Multi_Turn_Conversations|Роли в чатах и многоэтапные диалоги]].

### Пример 1: Классификация настроения

#### Zero-shot подход

```python
prompt = """Классифицируй настроение от 1 до 5 (плохо-хорошо) в следующих заявлениях:
- Еда отличная, но я ел получше
- Моя еда пришла с опозданием, но напитки очень вкусные"""

response = client.chat.completions.create(
    model=os.getenv("BASE_MODEL"),
    messages=[
        {"role": "user", "content": prompt}
    ]
)

print(response.choices[0].message.content)
```

**Результат:**
```
1. «Еда отличная, но я ел получше» - Оценка: 4
2. «Моя еда пришла с опозданием, но напитки очень вкусные» - Оценка: 3
```

#### One-shot подход

Добавим пример модели прежде, чем она приступит к классификации:

```python
prompt = """Классифицируй настроение от 1 до 5 (плохо-хорошо) в следующих заявлениях:
- Обслуживание было очень долгим - 1
- Еда отличная, но я ел получше -
- Моя еда пришла с опозданием, но напитки очень вкусные - """

response = client.chat.completions.create(
    model=os.getenv("BASE_MODEL"),
    messages=[
        {"role": "user", "content": prompt}
    ]
)

print(response.choices[0].message.content)
```

**Результат:**
```
- Еда отличная, но я ел получше — 4
- Моя еда пришла с опозданием, но напитки очень вкусные — 3
```

#### Few-shot подход

Предоставим несколько примеров:

```python
prompt = """Классифицируй настроение от 1 до 5 (плохо-хорошо) в следующих заявлениях:
- Стейк был просто превосходным - 5
- Все было нормально, без жалоб - 3
- Обслуживание было очень долгим - 1
- Еда отличная, но я ел получше -
- Моя еда пришла с опозданием, но напитки очень вкусные - """

response = client.chat.completions.create(
    model=os.getenv("BASE_MODEL"),
    messages=[
        {"role": "user", "content": prompt}
    ]
)

print(response.choices[0].message.content)
```

**Результат:**
```
- Еда отличная, но я ел получше — 4
- Моя еда пришла с опозданием, но напитки очень вкусные — 3
```

### Пример 2: Категоризация животных

#### Zero-shot подход

```python
prompt = """Классифицируй следующих животных на три группы: водные, земноводные и наземные
- Синий кит
- Полярный медведь
- Тунец
- Собака"""

response = client.chat.completions.create(
    model=os.getenv("BASE_MODEL"),
    messages=[
        {"role": "user", "content": prompt}
    ]
)

print(response.choices[0].message.content)
```

Как можно заметить, модель задает уточняющую информацию, по которой можно было бы присвоить ту или иную категорию. Попробуем указать примеры для задачи.

#### Few-shot подход

```python
prompt = """Классифицируй следующих животных на три группы: водные, земноводные и наземные
- Зебра - наземное
- Крокодил - земноводное
- Синий кит -
- Полярный медведь -
- Тунец -
- Собака - """

response = client.chat.completions.create(
    model=os.getenv("BASE_MODEL"),
    messages=[
        {"role": "user", "content": prompt}
    ]
)

print(response.choices[0].message.content)
```

**Результат:**
```
- Синий кит - водное
- Полярный медведь - земноводное
- Тунец - водное
- Собака - наземное
```

С предоставленными примерами модель дает более структурированные и предсказуемые ответы.

## Рекомендации по использованию

1. **Для редактирования текста**: Используйте четкие инструкции и явно выделяйте изменяемый текст
2. **Для резюмирования**: Указывайте желаемую длину или количество ключевых пунктов
3. **Для контроля стоимости**: Используйте параметр `max_completion_tokens` и отслеживайте использование токенов через `response.usage`
4. **Для творческих задач**: Увеличивайте параметр `temperature`
5. **Для точных и предсказуемых ответов**: Уменьшайте `temperature` до 0-0.3
6. **Для задач классификации**: Используйте few-shot prompting с примерами для каждой категории

## Практика

Для закрепления материала рекомендуется выполнить [[Exercises/01_OpenAI_API_Text_Processing_Exercise|Практическое задание №1]], которое охватывает все темы из этой заметки.

---

*Эта заметка создана на основе практических примеров работы с OpenAI API в Jupyter Notebook.*
